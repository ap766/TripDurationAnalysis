{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYdqmhDqed68TBjhvyzfjl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ap766/TripDurationAnalysis/blob/main/BDAproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjcwW3uswk4q",
        "outputId": "8818e2b3-216a-4c56-9c19-1f51c56f3173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGJ7LqU3z_9S",
        "outputId": "aa009c69-4579-4155-9564-ad6e578a9a24"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=ac4227590765e3924ff1276e79bfd371a2438f7e5ad2dd8ecbe83d9c1cef57af\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.regression import DecisionTreeRegressor, LinearRegression\n",
        "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
        "from pyspark.mllib.util import MLUtils\n",
        "from pyspark.sql import SparkSession, SQLContext, Row\n",
        "from pyspark.sql.functions import date_format, sin, cos, radians, atan2, month\n",
        "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "def distance(long1, lat1, long2, lat2):\n",
        "        radius = 6371\n",
        "        diff_lat = radians(lat2 - lat1)\n",
        "        diff_long = radians(long2 - long1)\n",
        "        a = sin(diff_lat/2)**2 + cos(lat1)*cos(lat2)*sin(diff_long/2)**2\n",
        "        c = 2*atan2(a**0.5, (1-a)**0.5)\n",
        "        return radius*c\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "        SparkContext.setSystemProperty(\"saprk.executor.memory\", \"12g\")\n",
        "        spark = SparkSession.builder.appName(\"RegressionTree\").getOrCreate()\n",
        "\n",
        "        # Load up data as dataframe\n",
        "        data = spark.read.csv(\"/content/drive/MyDrive/train.csv\", header=True)\n",
        "        # Data preprocessing\n",
        "        data = data.withColumn(\"pickup_longitude\", data[\"pickup_longitude\"].cast(\"float\")).withColumn(\"pickup_latitude\", data[\"pickup_latitude\"].cast(\"float\")).withColumn(\"dropoff_longitude\", data[\"dropoff_longitude\"].cast(\"float\")).withColumn(\"dropoff_latitude\", data[\"dropoff_latitude\"].cast(\"float\")).withColumn(\"passenger_count\", data[\"passenger_count\"].cast(\"int\")).withColumn(\"trip_duration\", data[\"trip_duration\"].cast(\"int\")).withColumn(\"pickup_datetime\", data[\"pickup_datetime\"].cast(\"timestamp\")).withColumn(\"dropoff_datetime\", data[\"dropoff_datetime\"].cast(\"timestamp\")).withColumn(\"vendor_id\", data[\"vendor_id\"].cast(\"int\"))\n",
        "\n",
        "        data = data.withColumn(\"pickup_weekday\", date_format(\"pickup_datetime\", \"E\")).withColumn(\"pickup_hour\", date_format(\"pickup_datetime\", \"H\")).withColumn(\"pickup_month\", date_format(\"pickup_datetime\", \"M\"))\n",
        "        data = data.withColumn(\"pickup_hour\", data[\"pickup_hour\"].cast(\"int\")).withColumn(\"pickup_month\", data[\"pickup_month\"].cast(\"int\"))\n",
        "        data = data.withColumn(\"dropoff_weekday\", date_format(\"dropoff_datetime\", \"E\")).withColumn(\"dropoff_hour\", date_format(\"dropoff_datetime\", \"H\")).withColumn(\"dropoff_month\", date_format(\"dropoff_datetime\", \"M\"))\n",
        "        data = data.withColumn(\"dropoff_hour\", data[\"dropoff_hour\"].cast(\"int\")).withColumn(\"dropoff_month\", data[\"dropoff_month\"].cast(\"int\"))\n",
        "        data = data.withColumn(\"trip_distance\", distance(data.pickup_longitude, data.pickup_latitude, data.dropoff_longitude, data.dropoff_latitude))\n",
        "        # Data cleaning\n",
        "        data = data.filter(data[\"trip_duration\"] > 10).filter(data[\"trip_duration\"] < 22*60*60).filter(data[\"pickup_longitude\"] <= -73.75).filter(data[\"pickup_longitude\"] >= -74.03).filter(data[\"dropoff_longitude\"] <= -73.75).filter(data[\"dropoff_longitude\"] >= -74.03).filter(data[\"pickup_latitude\"] <= 40.85).filter(data[\"pickup_latitude\"] >= 40.63).filter(data[\"dropoff_latitude\"] <= 40.85).filter(data[\"dropoff_latitude\"] >= 40.63)\n",
        "        #data.printSchema()\n",
        "        assembler = VectorAssembler().setInputCols([\"vendor_id\", \"pickup_longitude\", \"pickup_latitude\", \"pickup_hour\", \"pickup_month\", \"dropoff_longitude\", \"dropoff_latitude\", \"trip_distance\", \"passenger_count\"]).setOutputCol(\"features\")\n",
        "        df = assembler.setHandleInvalid(\"skip\").transform(data).select(\"trip_duration\", \"features\")\n",
        "\n",
        "        featureIndexer = VectorIndexer(inputCol = \"features\", outputCol = \"indexedFeatures\", maxCategories = 30).fit(df)\n",
        "        d = featureIndexer.transform(df)\n",
        "        trainTest = d.randomSplit([0.8, 0.2])\n",
        "        traindf = trainTest[0]\n",
        "        testdf = trainTest[1]\n",
        "\n",
        "        # Model\n",
        "        dtr = DecisionTreeRegressor(featuresCol=\"indexedFeatures\", labelCol=\"trip_duration\", impurity=\"variance\")\n",
        "\n",
        "        # choices of tuning parameters\n",
        "        dtrparamGrid = (ParamGridBuilder().addGrid(dtr.maxDepth, [10]).build())\n",
        "\n",
        "        pipeline = Pipeline(stages = [featureIndexer, dtr])\n",
        "\n",
        "        crossval = CrossValidator(estimator = pipeline, estimatorParamMaps = dtrparamGrid, evaluator = RegressionEvaluator(labelCol = \"trip_duration\", predictionCol = \"prediction\", metricName = \"rmse\"), numFolds = 10)\n",
        "        model = crossval.fit(traindf)\n",
        "\n",
        "        predictions = model.transform(testdf).cache()\n",
        "\n",
        "        predictions.show(25)\n",
        "\n",
        "        evaluator = RegressionEvaluator(labelCol=\"trip_duration\", predictionCol = \"prediction\", metricName = \"rmse\")\n",
        "        rmse = evaluator.evaluate(predictions)\n",
        "        print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
        "\n",
        "        with open(\"./test.txt\", \"a\") as f:\n",
        "                f.write(\"\\nHere is the result of RMSE with regression tree: \" + str(rmse))\n",
        "\n",
        "        evaluator2 = RegressionEvaluator(labelCol=\"trip_duration\", predictionCol = \"prediction\", metricName = \"mae\")\n",
        "        mae = evaluator2.evaluate(predictions)\n",
        "        print(\"Mean Absolute Error (MAE) on test data = %g\" % mae)\n",
        "\n",
        "        with open(\"./test.txt\", \"a\") as f:\n",
        "                f.write(\"\\nHere is the result of MAE with regression tree: \" + str(mae))\n",
        "        \"\"\" Here was the part to try enhanced decision tree\n",
        "        pred = predictions.select(\"prediction\").dropDuplicates()\n",
        "        for leaf_value in pred.collect():\n",
        "                input_df = predictions.filter(predictions[\"prediction\"] == leaf_value)\n",
        "                (train_input_df, test_input_df) = input_df.randomSplit([0.8, 0.2])\n",
        "                lr = LinearRegression()\n",
        "                model2 = lr.fit(train_input_df)\n",
        "                predictions2 = model2.transform(test_input_df)\n",
        "                predictions2.show(25)\n",
        "                rmse2 = evaluator.evaluate(predictions2)\n",
        "                with open(\"./test.txt\", \"a\") as f:\n",
        "                        f.write(\"\\n Here is the result of RMSE for current leaf\" + str(rmse2))\n",
        "\n",
        "                mae2 = evaluator2.evaluate(predictions2)\n",
        "                with open(\"./test.txt\", \"a\") as f:\n",
        "                        f.write(\"\\n Here is the result of MAE for current leaf\" + str(mae2))\n",
        "        \"\"\"\n",
        "        spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXW4BKA8x6Jm",
        "outputId": "eda2a564-5e32-4cb0-9bf8-b9b3db3d4d1b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+--------------------+--------------------+------------------+\n",
            "|trip_duration|            features|     indexedFeatures|        prediction|\n",
            "+-------------+--------------------+--------------------+------------------+\n",
            "|           11|[1.0,-74.00732421...|[0.0,-74.00732421...|307.13879191094134|\n",
            "|           11|[1.0,-74.00265502...|[0.0,-74.00265502...|243.53507484590548|\n",
            "|           11|[1.0,-74.00051879...|[0.0,-74.00051879...|362.14741970407795|\n",
            "|           11|[1.0,-73.99850463...|[0.0,-73.99850463...|307.13879191094134|\n",
            "|           11|[1.0,-73.97555541...|[0.0,-73.97555541...|  379.937358276644|\n",
            "|           11|[1.0,-73.94542694...|[0.0,-73.94542694...| 308.6702337369755|\n",
            "|           11|[1.0,-73.93783569...|[0.0,-73.93783569...|199.78828229027962|\n",
            "|           11|[1.0,-73.93550872...|[0.0,-73.93550872...|307.13879191094134|\n",
            "|           11|[1.0,-73.93496704...|[0.0,-73.93496704...|307.13879191094134|\n",
            "|           11|[1.0,-73.87580871...|[0.0,-73.87580871...|243.82051282051282|\n",
            "|           11|[1.0,-73.79040527...|[0.0,-73.79040527...|243.82051282051282|\n",
            "|           11|[1.0,-73.78355407...|[0.0,-73.78355407...| 573.0227272727273|\n",
            "|           11|[1.0,-73.77688598...|[0.0,-73.77688598...| 262.8977777777778|\n",
            "|           11|[2.0,-74.00289154...|[1.0,-74.00289154...|307.13879191094134|\n",
            "|           11|[2.0,-73.99105072...|[1.0,-73.99105072...|  379.937358276644|\n",
            "|           11|[2.0,-73.98983764...|[1.0,-73.98983764...| 262.8977777777778|\n",
            "|           11|[2.0,-73.98287200...|[1.0,-73.98287200...|  379.937358276644|\n",
            "|           11|[2.0,-73.98105621...|[1.0,-73.98105621...|243.53507484590548|\n",
            "|           11|[2.0,-73.98049163...|[1.0,-73.98049163...| 282.0713223518646|\n",
            "|           11|[2.0,-73.96408843...|[1.0,-73.96408843...| 308.6702337369755|\n",
            "|           11|[2.0,-73.90306854...|[1.0,-73.90306854...|             188.9|\n",
            "|           12|[1.0,-74.00936126...|[0.0,-74.00936126...|362.14741970407795|\n",
            "|           12|[1.0,-74.00830841...|[0.0,-74.00830841...| 282.0713223518646|\n",
            "|           12|[1.0,-73.99875640...|[0.0,-73.99875640...| 282.0713223518646|\n",
            "|           12|[1.0,-73.94494628...|[0.0,-73.94494628...| 282.0713223518646|\n",
            "+-------------+--------------------+--------------------+------------------+\n",
            "only showing top 25 rows\n",
            "\n",
            "Root Mean Squared Error (RMSE) on test data = 707.662\n",
            "Mean Absolute Error (MAE) on test data = 232.838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f=predictions.select(\"features\")\n",
        "f.show(5)"
      ],
      "metadata": {
        "id": "IJnnIBCWz2yK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "3ac5b354-c5a7-497d-a026-75ae83d6d1f3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "SparkContext or SparkSession should be created first.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a53f818ea345>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3225\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m         \"\"\"\n\u001b[0;32m-> 3227\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3228\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_jcols\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2763\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2764\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2766\u001b[0m     def _sort_cols(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_jseq\u001b[0;34m(self, cols, converter)\u001b[0m\n\u001b[1;32m   2749\u001b[0m     ) -> JavaObject:\n\u001b[1;32m   2750\u001b[0m         \u001b[0;34m\"\"\"Return a JVM Seq of Columns from a list of Column or names\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2753\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_jmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJavaObject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_seq\u001b[0;34m(sc, cols, converter)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \"\"\"\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \"\"\"\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mjcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mjcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_column_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         raise PySparkTypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_create_column_from_name\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_create_column_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Column\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_active_spark_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJVMView\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mget_active_spark_context\u001b[0;34m()\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SparkContext or SparkSession should be created first.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: SparkContext or SparkSession should be created first."
          ]
        }
      ]
    }
  ]
}